# Evaluation configuration
# This file contains settings for model evaluation

# Evaluation parameters
batch_size: 8
num_workers: 4
device: "auto"

# Metrics to compute
metrics:
  - iou
  - dice
  - precision
  - recall
  - f1
  - accuracy

# Threshold for binary classification
threshold: 0.5

# Save predictions
save_predictions: true
save_visualizations: true
save_metrics: true

# Prediction output format
prediction_format: "numpy"  # numpy, torch, pil

# Visualization settings
visualization:
  num_samples: 10
  save_format: "png"
  show_ground_truth: true
  show_predictions: true
  show_overlay: true

# Evaluation dataset settings
dataset:
  split: "test"
  shuffle: false
  drop_last: false

# Output directory for evaluation results
save_dir: "artifacts/experiments"      # Directory to save evaluation outputs (predictions, metrics, etc.)

# Evaluation mode
mode: "test"  # test, validation

# Additional evaluation settings
additional_settings:
  compute_confusion_matrix: true
  save_detailed_metrics: true
  export_results: true