defaults:
  - _self_
  - bottleneck: aspp_bottleneck  # Select ASPP as the bottleneck (see configs/model/bottleneck/aspp_bottleneck.yaml)

# Target the main UNet assembly class
_target_: src.model.unet.BaseUNet

# --- Component Configurations ---

encoder:
  in_channels: 3    # Example: For RGB input
  init_features: 64 # Number of features after the first conv layer
  depth: 4          # Number of downsampling blocks
  _target_: src.model.encoder.cnn_encoder.CNNEncoder

bottleneck:
  in_channels: 512
  out_channels: 1024 # Example: Double channels in bottleneck
  dropout: 0.5
  _target_: src.model.bottleneck.cnn_bottleneck.BottleneckBlock

decoder:
  in_channels: 1024
  # Este skip_channels_list debe coincidir con encoder.skip_channels para evitar errores
  # El orden debe ser desde alta resolución (cercana a la entrada) hasta baja resolución 
  # (cercana al bottleneck), lo que coincide con el orden en que el encoder produce los skips.
  # CNNDecoder internamente invertirá esta lista al usarla.
  skip_channels_list: [64, 128, 256, 512]
  out_channels: 1
  depth: 4 # Must match encoder depth
  _target_: src.model.decoder.cnn_decoder.CNNDecoder

  # --- Optional CBAM Attention Block ---
  # Enable CBAM attention in each decoder block (default: false)
  cbam_enabled: false  # Set to true to enable CBAM in decoder blocks
  cbam_params:
    reduction: 16      # Reduction ratio for channel attention (default: 16)
    kernel_size: 7     # Kernel size for spatial attention (default: 7)

# --- Optional Final Activation ---
# final_activation:
#   _target_: torch.nn.Sigmoid
#   # Add any necessary parameters for the activation function here 