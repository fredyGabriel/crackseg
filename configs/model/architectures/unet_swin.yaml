# @package _group_

# U-Net architecture with Swin Transformer V2 Encoder
# Uses Swin Transformer V2 as encoder, paired with CNN-based bottleneck and decoder for segmentation

_target_: src.model.unet.BaseUNet  # Main UNet class to instantiate
type: BaseUNet

# --- Component Configurations ---
encoder:
  _target_: src.model.factory.create_encoder  # Factory function for encoder
  type: "SwinTransformerEncoder"             # Use Swin Transformer as encoder
  in_channels: 3                             # Number of input channels (e.g., RGB)
  model_name: "swinv2_tiny_window16_256"     # SwinV2 model variant
  pretrained: true                           # Use ImageNet pretrained weights
  img_size: 256                              # Input image size for SwinV2
  patch_size: 4                              # Patch size for Swin Transformer
  handle_input_size: "resize"                # Resize input images for compatibility
  output_hidden_states: true                  # Output all hidden states
  features_only: true                        # Only output feature maps
  out_indices: [0, 1, 2, 3]                  # Indices of feature maps to use as skips
  output_norm: true                          # Apply normalization to output features

bottleneck:
  _target_: src.model.factory.create_bottleneck  # Factory function for bottleneck
  type: "CNNBottleneckBlock"                    # Use CNN bottleneck
  # For swinv2_tiny_window16_256, feature channels are [96, 192, 384, 768]
  # Bottleneck receives last feature map (768 channels)
  in_channels: 768
  out_channels: 1024

decoder:
  _target_: src.model.factory.create_decoder     # Factory function for decoder
  type: "CNNDecoder"                           # Use CNN decoder
  in_channels: 1024                             # Match bottleneck output channels
  # Skip connection channels for swinv2_tiny_window16_256 (reverse order)
  # [384, 192, 96] from encoder's skip_channels
  skip_channels_list: [384, 192, 96]
  out_channels: 1                               # Binary segmentation by default
  depth: 3                                      # Swin model has 3 skip connections

# Optional sigmoid activation for binary segmentation
final_activation:
  _target_: torch.nn.Sigmoid

training:
  optimizer:
    _target_: torch.optim.AdamW                 # AdamW optimizer for Transformers
    lr: 0.0001                                  # Lower learning rate for pretrained models
    weight_decay: 0.01                          # Higher weight decay for regularization 