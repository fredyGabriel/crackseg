# @package _group_

# U-Net architecture with Swin Transformer V2 Encoder
# This model uses a Swin Transformer V2 as encoder, paired with CNN-based
# bottleneck and decoder for segmentation tasks

_target_: src.model.unet.BaseUNet
type: BaseUNet

# --- Component Configurations ---
encoder:
  # Use the factory function to create the encoder
  _target_: src.model.factory.create_encoder
  # Specify the SwinTransformerEncoder
  type: "SwinTransformerEncoder"
  # Basic parameters
  in_channels: 3
  # Use a smaller Swin Transformer V2 model by default
  model_name: "swinv2_tiny_window16_256"
  pretrained: true
  # Default to using img_size matching the model name
  img_size: 256
  patch_size: 4
  # Resize input images by default for compatibility
  handle_input_size: "resize"
  # Feature extraction setup
  output_hidden_states: true
  features_only: true
  out_indices: [0, 1, 2, 3]
  # Output processing
  output_norm: true

bottleneck:
  # Use the factory function to create the bottleneck
  _target_: src.model.factory.create_bottleneck
  type: "CNNBottleneckBlock"
  # Use appropriate channel sizes for Swin Tiny model
  # For swinv2_tiny_window16_256, the feature channels are [96, 192, 384, 768]
  # The bottleneck receives the last feature map with 768 channels
  in_channels: 768
  out_channels: 1024

decoder:
  # Use the factory function to create the decoder
  _target_: src.model.factory.create_decoder
  type: "CNNDecoder"
  in_channels: 1024  # Match bottleneck output channels
  # Skip connection channels for swinv2_tiny_window16_256 (in reverse order)
  # [384, 192, 96] - these channels come from the encoder's skip_channels
  skip_channels_list: [384, 192, 96]
  out_channels: 1  # Binary segmentation by default
  depth: 3  # Swin model has 3 skip connections for decoder

# Optional sigmoid activation for binary segmentation
final_activation:
  _target_: torch.nn.Sigmoid

training:
  optimizer:
    _target_: torch.optim.AdamW  # AdamW works better with Transformers
    lr: 0.0001  # Lower learning rate for pretrained Transformer models
    weight_decay: 0.01  # Higher weight decay for regularization 