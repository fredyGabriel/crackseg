# configs/training/trainer.yaml

defaults:
  # - lr_scheduler: step_lr  # Cambia a cosine o reduce_on_plateau seg√∫n necesidad
  - _self_

# Default configuration for the Trainer class

_target_: src.training.trainer.Trainer # Target class for instantiation

# Basic Training Parameters
# epochs: 10
# Note: batch_size is typically defined in the dataloader config
# but can be referenced here if needed.

# Device settings
device: "auto" # Options: "auto", "cpu", "cuda", "cuda:0", etc.

# Optimizer settings (Example - define actual optimizer in separate config)
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001

# Learning Rate Scheduler settings (Optional - define in separate config)
lr_scheduler: ${lr_scheduler}

# Mixed Precision Training (AMP)
use_amp: true # Enable/disable Automatic Mixed Precision

# Gradient Accumulation
gradient_accumulation_steps: 1 # Number of steps to accumulate gradients over (1 = disabled)

# Checkpoint settings
checkpoint_dir: "outputs/checkpoints"
save_freq: 0  # How often to save intermediate checkpoints (epochs), 0 = disable

# Save best model configuration
save_best:
  enabled: true
  monitor_metric: "val_loss"  # Metric to monitor (must match key in validation results)
  monitor_mode: "min"  # 'min' for minimizing (e.g. loss), 'max' for maximizing (e.g. accuracy)
  best_filename: "model_best.pth.tar"

# Early Stopping settings
early_stopping:
  _target_: src.utils.early_stopping.EarlyStopping
  monitor: "val_loss"
  patience: 2
  mode: "min"
  min_delta: 0.001
  verbose: true

# Verbosity / Progress reporting
verbose: true
progress_bar: true
log_interval_batches: 50 # Log training batch loss every N batches (0 to disable)