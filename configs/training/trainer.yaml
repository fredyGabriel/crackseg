# configs/training/trainer.yaml - CONFIGURED FOR BASIC VERIFICATION

defaults:
  # - lr_scheduler: step_lr  # Change to cosine or reduce_on_plateau as needed
  - _self_

# Default configuration for the Trainer class

_target_: src.training.trainer.Trainer  # Target Trainer class for instantiation

# Basic Training Parameters
# epochs: 10
# Note: batch_size is typically defined in the dataloader config
# but can be referenced here if needed.

# Device settings
device: "auto"  # Options: "auto", "cpu", "cuda", "cuda:0", etc.

# Optimizer settings (Example - define actual optimizer in separate config)
optimizer:
  _target_: torch.optim.Adam  # Optimizer class
  lr: 0.001                  # Learning rate

# Learning Rate Scheduler settings (Optional - define in separate config)
lr_scheduler: ${lr_scheduler}  # Reference to scheduler config

# Mixed Precision Training (AMP) - DISABLED for verification stability
use_amp: false  # Enable/disable Automatic Mixed Precision

# Gradient Accumulation
gradient_accumulation_steps: 1  # Number of steps to accumulate gradients (1 = disabled)

# Checkpoint settings
checkpoint_dir: "outputs/checkpoints"  # Directory to save checkpoints
save_freq: 0  # How often to save intermediate checkpoints (epochs), 0 = disable

# Save best model configuration
save_best:
  enabled: true
  monitor_metric: "val_loss"  # Metric to monitor (must match key in validation results)
  monitor_mode: "min"         # 'min' for minimizing (e.g. loss), 'max' for maximizing (e.g. accuracy)
  best_filename: "model_best.pth.tar"

# Early Stopping settings - RELAXED for basic verification
early_stopping:
  _target_: src.utils.early_stopping.EarlyStopping  # EarlyStopping class
  monitor: "val_loss"      # Metric to monitor for early stopping
  patience: 5              # Number of epochs with no improvement to wait - INCREASED
  mode: "min"              # 'min' for loss, 'max' for accuracy/metric
  min_delta: 0.01          # Minimum change to qualify as improvement - RELAXED
  verbose: true            # Print early stopping messages

# Verbosity / Progress reporting
verbose: true
progress_bar: true
log_interval_batches: 10  # Log training batch loss every N batches (REDUCED for more feedback)