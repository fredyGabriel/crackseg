defaults:
  - trainer
  - lr_scheduler: step_lr      # Select default learning rate scheduler
  - loss: bce_dice             # Select default loss config from loss/ group
  - _self_

# Default training configuration - CONFIGURED FOR BASIC VERIFICATION
# Edit these values for your training process

epochs: 2                      # Number of training epochs - FAST for verification
learning_rate: 0.001           # Initial learning rate
# optimizer: adam              # Options: adam, sgd, etc. (Need factory/config for this too)
weight_decay: 0.0001           # Weight decay (L2 regularization)
scheduler: step_lr             # Learning rate scheduler (step_lr, cosine, etc.)
step_size: 10                  # Step size for step_lr scheduler
gamma: ${thresholds.gamma}     # Decay factor for step_lr scheduler

device: "auto"                 # Device to use for training (auto = GPU if available, else CPU)

# Loss configuration is now handled by the 'loss' default group above
# loss:
#  _target_: src.training.losses.BCEDiceLoss
#  bce_weight: 0.5
#  dice_weight: 0.5
#  smooth: 1.0