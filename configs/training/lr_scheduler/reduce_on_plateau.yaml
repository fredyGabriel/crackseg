# ReduceLROnPlateau scheduler config
_target_: torch.optim.lr_scheduler.ReduceLROnPlateau  # Reduce LR on plateau scheduler class
mode: min         # 'min' for loss, 'max' for metrics
factor: 0.1       # Factor to reduce LR by when plateau is reached
patience: 5       # Number of epochs with no improvement before reducing LR
threshold: 0.0001 # Minimum change to qualify as improvement
min_lr: 0.0       # Minimum learning rate allowed 