name: Code Quality Gates

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'tests/**'
      - '.github/workflows/quality-gates.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'tests/**'

jobs:
  quality-gates-core:
    name: Core Quality Gates
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash -el {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Conda Environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          activate-environment: crackseg
          environment-file: environment.yml
          auto-activate-base: false

      - name: üîç Ruff Linting - Core Code
        run: |
          echo "üîç Running Ruff linter on core codebase..."
          python -m ruff check src/ --format github
          if [ $? -ne 0 ]; then
            echo "‚ùå Ruff linting failed for core code"
            exit 1
          fi
          echo "‚úÖ Core code linting passed"

      - name: üé® Black Formatting - Core Code
        run: |
          echo "üé® Checking Black formatting on core codebase..."
          black src/ --check --diff --color
          if [ $? -ne 0 ]; then
            echo "‚ùå Black formatting failed for core code"
            exit 1
          fi
          echo "‚úÖ Core code formatting passed"

      - name: üî¨ Type Checking - Core Code
        run: |
          echo "üî¨ Running Basedpyright type checking on core codebase..."
          basedpyright src/
          if [ $? -ne 0 ]; then
            echo "‚ùå Type checking failed for core code"
            exit 1
          fi
          echo "‚úÖ Core code type checking passed"

  quality-gates-gui:
    name: GUI Quality Gates
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash -el {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Conda Environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          activate-environment: crackseg
          environment-file: environment.yml
          auto-activate-base: false

      - name: üîç Ruff Linting - GUI Code
        run: |
          echo "üîç Running Ruff linter on GUI codebase..."
          python -m ruff check scripts/gui/ --format github
          if [ $? -ne 0 ]; then
            echo "‚ùå Ruff linting failed for GUI code"
            echo "üí° Run 'conda activate crackseg && python -m ruff check scripts/gui/ --fix' locally to fix issues"
            exit 1
          fi
          echo "‚úÖ GUI code linting passed"

      - name: üé® Black Formatting - GUI Code
        run: |
          echo "üé® Checking Black formatting on GUI codebase..."
          black scripts/gui/ --check --diff --color
          if [ $? -ne 0 ]; then
            echo "‚ùå Black formatting failed for GUI code"
            echo "üí° Run 'conda activate crackseg && black scripts/gui/' locally to fix formatting"
            exit 1
          fi
          echo "‚úÖ GUI code formatting passed"

      - name: üî¨ Type Checking - GUI Code
        run: |
          echo "üî¨ Running Basedpyright type checking on GUI codebase..."
          basedpyright scripts/gui/
          if [ $? -ne 0 ]; then
            echo "‚ùå Type checking failed for GUI code"
            echo "üí° Review type annotations and imports in GUI code"
            exit 1
          fi
          echo "‚úÖ GUI code type checking passed"

      - name: üìä GUI Quality Report
        if: always()
        run: |
          echo "üìä Generating GUI Quality Report..."
          echo "## üñ•Ô∏è GUI Code Quality Summary" >> $GITHUB_STEP_SUMMARY

          # Count GUI files
          gui_files=$(find scripts/gui -name "*.py" | wc -l)
          echo "- **Total GUI Python files**: $gui_files" >> $GITHUB_STEP_SUMMARY

          # Ruff check summary
          if python -m ruff check scripts/gui/ --quiet; then
            echo "- **Ruff Linting**: ‚úÖ All checks passed" >> $GITHUB_STEP_SUMMARY
          else
            ruff_issues=$(python -m ruff check scripts/gui/ --format json | jq length)
            echo "- **Ruff Linting**: ‚ùå $ruff_issues issues found" >> $GITHUB_STEP_SUMMARY
          fi

          # Black check summary
          if black scripts/gui/ --check --quiet; then
            echo "- **Black Formatting**: ‚úÖ All files properly formatted" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Black Formatting**: ‚ùå Files need formatting" >> $GITHUB_STEP_SUMMARY
          fi

          # Type check summary
          if basedpyright scripts/gui/ --outputjson > type_check.json 2>/dev/null; then
            errors=$(jq '.summary.errorCount // 0' type_check.json)
            warnings=$(jq '.summary.warningCount // 0' type_check.json)
            echo "- **Type Checking**: ‚úÖ $errors errors, $warnings warnings" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Type Checking**: ‚ùå Type check failed" >> $GITHUB_STEP_SUMMARY
          fi

  test-coverage:
    name: Test Coverage
    runs-on: ubuntu-latest
    needs: [quality-gates-core, quality-gates-gui]
    defaults:
      run:
        shell: bash -el {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Conda Environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          activate-environment: crackseg
          environment-file: environment.yml
          auto-activate-base: false

      - name: üß™ Run Tests with Coverage
        run: |
          echo "üß™ Running tests with coverage analysis..."
          pytest tests/unit/ tests/integration/ \
            --cov=src \
            --cov=scripts/gui \
            --cov-report=xml \
            --cov-report=json:coverage.json \
            --cov-report=term-missing \
            --cov-fail-under=80
          if [ $? -ne 0 ]; then
            echo "‚ùå Tests failed or coverage below 80%"
            exit 1
          fi
          echo "‚úÖ All tests passed with adequate coverage"

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

      - name: Comment on PR with Coverage Report
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const coverage = JSON.parse(fs.readFileSync('coverage.json', 'utf8'));
            const total_coverage = coverage.totals.percent_covered_display;
            const threshold = 80;
            const status = total_coverage >= threshold ? '‚úÖ' : '‚ùå';

            const body = `
            ## üìä Test Coverage Report

            | Metric          | Result        |
            |-----------------|---------------|
            | **Total Coverage** | \`${total_coverage}%\` |
            | **Threshold**      | \`${threshold}%\`      |
            | **Status**         | ${status}         |

            *Report generated for commit: ${github.sha}*
            `;

            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

      - name: Upload Coverage Reports as Artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: coverage-reports
          path: |
            coverage.xml
            coverage.json

  performance-benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [quality-gates-core, quality-gates-gui]
    defaults:
      run:
        shell: bash -el {0}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Conda Environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          activate-environment: crackseg
          environment-file: environment.yml
          auto-activate-base: false

      - name: üî¨ Run Performance Benchmarks
        run: |
          echo "üî¨ Running performance benchmarks..."
          pytest tests/ --benchmark-only --benchmark-json=benchmark_results.json
          if [ $? -ne 0 ]; then
            echo "‚ùå Performance benchmarks failed"
            exit 1
          fi
          echo "‚úÖ Performance benchmarks completed"

      - name: Upload Benchmark Results as Artifact
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark_results.json

  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [quality-gates-core, quality-gates-gui, test-coverage, performance-benchmark]
    if: failure()

    steps:
      - name: Send Slack Failure Notification
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_USERNAME: "CI Bot"
          SLACK_ICON_EMOJI: ":x:"
          SLACK_COLOR: "danger"
          SLACK_MESSAGE: "A quality gate failed in the '${{ github.ref_name }}' branch. Please review the logs."
          SLACK_TITLE: "CI Failure Alert"
          SLACK_FOOTER: "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"