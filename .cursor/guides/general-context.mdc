---
description: General context for what the project is being developed for
globs:
alwaysApply: false
---
# Project Overview: CrackSeg - SOTA Pavement Crack Segmentation

Advanced deep learning research project focused on achieving state-of-the-art performance in pavement crack segmentation using modular U-Net architectures and Transformer-based approaches.

**Professional Development**: This project follows strict quality standards defined in [.cursor/rules/](mdc:../../.cursor/rules) for code quality, testing, and ML research practices.

## Contents

- [Research Objectives](mdc:#research-objectives)
- [Technical Domain](mdc:#technical-domain)
- [Modular Architecture Strategy](mdc:#modular-architecture-strategy)
- [Performance Benchmarks](mdc:#performance-benchmarks)
- [Hardware Constraints](mdc:#hardware-constraints)
- [Development Standards](mdc:#development-standards)

## Research Objectives

### Primary Goal
Develop and evaluate modular U-Net architectures to achieve state-of-the-art performance in pavement crack segmentation, with emphasis on:

- **Modular Design**: Interchangeable encoder/decoder components following Abstract Base Classes (ABCs)
- **SOTA Performance**: Target IoU > 0.8 on standard benchmarks
- **Reproducible Research**: Following [ML research standards](mdc:../../.cursor/rules/ml-research-standards.mdc)
- **Professional Code Quality**: Adhering to [coding standards](mdc:../../.cursor/rules/coding-preferences.mdc)

### Research Questions
1. Which encoder-decoder combinations achieve optimal crack detection performance?
2. How do Transformer-based encoders (SwinV2) compare to CNN approaches for crack segmentation?
3. What attention mechanisms provide the most benefit for fine-grained crack features?
4. How can we optimize performance within 8GB VRAM constraints?

## Technical Domain

### Problem Specification
- **Task**: Semantic segmentation (pixel-level binary classification)
- **Input**: RGB images of asphalt surfaces (512√ó512 pixels)
- **Output**: Binary segmentation masks (crack vs. background)
- **Challenge**: High class imbalance (cracks are minority class)

### Datasets
```python
# Data structure following project standards
data/
‚îú‚îÄ‚îÄ train/          # Training split
‚îú‚îÄ‚îÄ val/            # Validation split
‚îî‚îÄ‚îÄ test/           # Test split
    ‚îú‚îÄ‚îÄ images/     # Input RGB images (512√ó512)
    ‚îî‚îÄ‚îÄ masks/      # Binary masks (512√ó512)
```

**Available Datasets**:
- **SUT Dataset**: 130 image/mask pairs (baseline)
- **Custom Dataset**: ~1000 image/mask pairs (internal validation)
- **Public Augmentation**: Crack500, CFD, DeepCrack (generalization)

### Evaluation Metrics
Following [ML standards](mdc:../../.cursor/rules/ml-research-standards.mdc):
- **Primary**: Intersection over Union (IoU), F1-Score
- **Secondary**: Precision, Recall, Accuracy
- **Reproducibility**: Multiple runs with different seeds (reported as mean ¬± std)

## Modular Architecture Strategy

### Core Framework
```python
# Abstract architecture following coding-preferences.mdc
from abc import ABC, abstractmethod
import torch.nn as nn

class BaseEncoder(nn.Module, ABC):
    """Abstract base class for all encoder implementations."""

    @abstractmethod
    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        """Return multi-scale features for skip connections."""
        pass

class BaseDecoder(nn.Module, ABC):
    """Abstract base class for all decoder implementations."""

    @abstractmethod
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """Reconstruct segmentation from multi-scale features."""
        pass
```

### Component Categories

#### 1. Encoder Backbones
- **CNN-based**: ResNet, EfficientNet variants (pre-trained)
- **Transformer-based**: Swin Transformer V2 (Tiny variant for VRAM efficiency)
- **Hybrid**: Combined CNN-Transformer approaches

#### 2. Bottleneck Modules
- **Standard**: Convolutional blocks with batch normalization
- **ASPP**: Atrous Spatial Pyramid Pooling for multi-scale context
- **ConvLSTM**: Spatial-temporal modeling (experimental)

#### 3. Decoder Architectures
- **CNN**: Standard upsampling + convolution blocks
- **Transformer**: Swin V2 blocks for symmetric encoder-decoder
- **Attention-enhanced**: CBAM integration for feature refinement

#### 4. Skip Connection Strategies
- **Standard**: Feature concatenation
- **Attention Gates**: Selective feature focusing
- **CBAM-enhanced**: Channel and spatial attention

### Planned Architecture Variants

```yaml
# Example configuration (configs/model/architectures/)
architectures:
  1_cnn_unet:
    encoder: resnet34
    bottleneck: standard_conv
    decoder: cnn_decoder

  2_swin_unet:
    encoder: swin_v2_tiny
    bottleneck: aspp
    decoder: cnn_decoder

  3_hybrid_attention:
    encoder: swin_v2_tiny
    bottleneck: aspp
    decoder: attention_decoder
    skip_attention: cbam
```

## Performance Benchmarks

### SOTA Targets
Based on literature review and survey analysis:

| Metric | Baseline | Target | SOTA Goal |
|--------|----------|--------|-----------|
| IoU | 0.65-0.75 | > 0.80 | > 0.85 |
| F1-Score | 0.70-0.80 | > 0.85 | > 0.90 |
| Precision | Variable | > 0.85 | > 0.90 |
| Recall | Variable | > 0.80 | > 0.85 |

### Evaluation Protocol
Following [testing standards](mdc:../../.cursor/rules/testing-standards.mdc):

```python
# Standard evaluation pipeline
def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Dict[str, float]:
    """Evaluate model following project standards."""
    metrics = {
        'iou': [],
        'f1_score': [],
        'precision': [],
        'recall': []
    }

    # Implementation with proper typing and validation
    # See: src/evaluation/metrics.py
```

### Benchmark Datasets
- **Primary**: Public datasets (SUT, Crack500) for SOTA comparison
- **Secondary**: Custom dataset for internal validation
- **Reproducibility**: Fixed splits, standardized preprocessing

## Hardware Constraints

### Computational Environment
- **GPU**: NVIDIA RTX 3070 Ti Laptop (8GB GDDR6 VRAM)
- **CPU**: Intel Core i7-12650H (12th Gen)
- **RAM**: 32GB DDR4
- **OS**: Windows 11

### VRAM Optimization Strategies
Following [ML research standards](mdc:../../.cursor/rules/ml-research-standards.mdc):

```python
# Example optimization configuration
training:
  batch_size: 4                    # Optimized for 8GB VRAM
  mixed_precision: true            # AMP for memory efficiency
  gradient_accumulation_steps: 4   # Effective batch size: 16
  gradient_checkpointing: true     # Trade compute for memory

model:
  encoder:
    variant: swin_v2_tiny         # Smallest Transformer variant
    pretrained: true              # Transfer learning
  decoder:
    channels: [256, 128, 64, 32]  # Progressive channel reduction
```

### Performance Monitoring
```bash
# VRAM monitoring during training
python run.py training.log_gpu_memory=true \
             training.mixed_precision=true \
             data.batch_size=4
```

## Development Standards

### Code Quality Integration
This project integrates with our professional development standards:

- **[Code Quality](mdc:../../.cursor/rules/coding-preferences.mdc)**: Mandatory type hints, Black formatting, Ruff linting
- **[Testing](mdc:../../.cursor/rules/testing-standards.mdc)**: >80% coverage, ML-specific test strategies
- **[Git Workflow](mdc:../../.cursor/rules/git-standards.mdc)**: Conventional commits, descriptive experiment documentation
- **[ML Research](mdc:../../.cursor/rules/ml-research-standards.mdc)**: Reproducibility, VRAM optimization, experiment tracking

### Quality Verification Workflow
```bash
# Pre-commit quality checks (mandatory)
black .                    # Auto-formatting
ruff . --fix              # Linting with autofix
basedpyright .            # Type checking
pytest tests/ --cov=src   # Test coverage

# ML-specific validation
python -m src.evaluation.validate_configs
python -m src.utils.memory_profiler
```

### Research Documentation Standards
Each experiment must include:

1. **Configuration**: Complete Hydra configs with hyperparameters
2. **Results**: Metrics with statistical significance testing
3. **Reproducibility**: Seeds, environment details, exact commands
4. **Comparison**: Baseline comparison and SOTA positioning
5. **Code Quality**: All quality gates passed

---

## Quick References

### üìö Essential Documentation
- **Architecture Guide**: [structural-guide.mdc](mdc:structural-guide.mdc)
- **Development Process**: [development-guide.mdc](mdc:development-guide.mdc)
- **Project Structure**: [project-structure.mdc](mdc:project-structure.mdc)
- **Glossary**: [glossary.mdc](mdc:glossary.mdc)

### üõ†Ô∏è Core Rules
- **Coding Standards**: [coding-preferences.mdc](mdc:../../.cursor/rules/coding-preferences.mdc)
- **Testing Guidelines**: [testing-standards.mdc](mdc:../../.cursor/rules/testing-standards.mdc)
- **ML Research**: [ml-research-standards.mdc](mdc:../../.cursor/rules/ml-research-standards.mdc)
- **Git Workflow**: [git-standards.mdc](mdc:../../.cursor/rules/git-standards.mdc)

---

**This document provides the technical foundation for professional crack segmentation research following industrial ML development standards.**