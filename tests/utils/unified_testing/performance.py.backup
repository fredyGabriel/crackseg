"""
Performance testing module for unified testing framework. This module
consolidates performance testing functionality from the original
frameworks, providing unified performance profiling and regression
testing.
"""

import  time
from  collections.abc  import  Callable
from  typing  import   Any

from  .core  import  PerformanceProfile


class UnifiedPerformanceTester:
    """
Unified performance testing consolidating all performance
functionality.
"""
def __init__(self) -> None: self._profiles: dict[str,
list[PerformanceProfile]] = {} def profile_component_render(self,
component_name: str, render_func: Callable[..., Any], *args: Any,
**kwargs: Any, ) -> PerformanceProfile:
"""Profile a component's rendering performance."""
import os import psutil process = psutil.Process(os.getpid())
memory_before = process.memory_info().rss / 1024 / 1024 # MB
start_time = time.time() result = render_func(*args, **kwargs)
render_time = (time.time() - start_time) * 1000 # Convert to ms
memory_after = process.memory_info().rss / 1024 / 1024 # MB
memory_usage = memory_after - memory_before # Estimate component
complexity widget_count = self._estimate_widget_count(result)
complexity_score = self._calculate_complexity_score( render_time,
memory_usage, widget_count ) profile = PerformanceProfile(
component_name=component_name, render_time_ms=render_time,
memory_usage_mb=memory_usage, interaction_latency_ms=0.0, # Will be
measured separately widget_count=widget_count,
complexity_score=complexity_score, ) if component_name not in
self._profiles: self._profiles[component_name] = []
self._profiles[component_name].append(profile) return profile def
get_performance_baseline(self, component_name: str ) -> dict[str,
float] | None:
"""Get performance baseline for a component."""
        if component_name not in self._profiles:
            return None

        profiles = self._profiles[component_name]
        if not profiles:
            return None

        render_times: list[float] = [p.render_time_ms for p in profiles]
        memory_usages: list[float] = [p.memory_usage_mb for p in profiles]

        return {
            "mean_render_time_ms": sum(render_times) / len(render_times),
            "max_render_time_ms": max(render_times),
            "mean_memory_usage_mb": sum(memory_usages) / len(memory_usages),
            "max_memory_usage_mb": max(memory_usages),
        }

    def assert_performance_regression(self,
        component_name: str,
        current_profile: PerformanceProfile,
        tolerance_percent: float = 20.0,
    ) -> bool:
        """Assert that performance hasn't regressed beyond tolerance."""
        baseline = self.get_performance_baseline(component_name)
        if not baseline:
            # First run - establish baseline
            return True

        render_time_increase = (
            (current_profile.render_time_ms - baseline["mean_render_time_ms"])
            / baseline["mean_render_time_ms"]
            * 100
        )

        memory_increase = (
            (
                current_profile.memory_usage_mb
                - baseline["mean_memory_usage_mb"]
            )
            / baseline["mean_memory_usage_mb"]
            * 100
        )

        if (
            render_time_increase > tolerance_percent
            or memory_increase > tolerance_percent
        ):
            return False

        return True

    def benchmark_component_render(self, component_func: Callable[..., Any], iterations: int = 100
    ) -> dict[str, float]:
        """Benchmark component rendering performance (legacy compatibility)."""
        render_times: list[float] = []

        for _ in range(iterations):
            start_time = time.time()
            try:
                component_func()
            except Exception:
                pass  # Ignore errors for performance testing
            render_times.append(
                (time.time() - start_time) * 1000
            )  # Convert to ms

        return {
            "mean_time_ms": sum(render_times) / len(render_times),
            "min_time_ms": min(render_times),
            "max_time_ms": max(render_times),
            "total_time_ms": sum(render_times),
        }

    def measure_memory_usage(self, func: Callable[..., Any]) -> dict[str, Any]:
        """Measure memory usage during function execution."""
        import  os

        import  psutil

        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss

        start_time = time.time()
        result = func()
        execution_time = time.time() - start_time

        memory_after = process.memory_info().rss
        memory_diff = memory_after - memory_before

        return {
            "result": result,
            "execution_time": execution_time,
            "memory_before_mb": memory_before / 1024 / 1024,
            "memory_after_mb": memory_after / 1024 / 1024,
            "memory_diff_mb": memory_diff / 1024 / 1024,
        }

    def get_all_profiles(self) -> dict[str, list[PerformanceProfile]]:
        """Get all performance profiles."""
return self._profiles.copy() def clear_profiles(self, component_name:
str | None = None) -> None:
"""Clear performance profiles for a component or all components."""
if component_name: if component_name in self._profiles:
self._profiles[component_name].clear() else: self._profiles.clear()
def _estimate_widget_count(self, render_result: Any) -> int:
"""Estimate number of widgets in render result."""
# Simple heuristic - count common widget patterns # In real
implementation, would analyze the mock calls to count widgets return 1
def _calculate_complexity_score(self, render_time: float,
memory_usage: float, widget_count: int ) -> float:
"""Calculate a complexity score for the component."""
        time_score = min(render_time / 100.0, 10.0)  # Normalize to 0-10
        memory_score = min(memory_usage / 10.0, 10.0)  # Normalize to 0-10
        widget_score = min(widget_count / 5.0, 10.0)  # Normalize to 0-10

        return (time_score + memory_score + widget_score) / 3.0
