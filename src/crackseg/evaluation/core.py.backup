from  typing  import   Any

import  torch
from  omegaconf  import  DictConfig
from  torch.utils.data  import  DataLoader


def evaluate_model(model: torch.nn.Module,
    dataloader: DataLoader[Any],
    metrics: dict[str, Any],
    device: torch.device,
    config: DictConfig,
) -> tuple[dict[str, float], tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:
    """
Evaluate a model on a dataset. Args: model: Model to evaluate
dataloader: DataLoader with evaluation data metrics: Dictionary of
metric functions device: Device to use for evaluation config:
Configuration object Returns: Tuple containing: - Dictionary of metric
results - Tuple of (inputs, targets, outputs) tensors for
visualization
"""
    if len(dataloader) == 0:
        results = {f"test_{name}": 0.0 for name in metrics.keys()}
        results["test_loss"] = 0.0
        empty = torch.empty(0)
        return results, (empty, empty, empty)

    model.eval()
    results = {f"test_{name}": 0.0 for name in metrics.keys()}
    # Add loss placeholder even though we may not calculate it
    results["test_loss"] = 0.0

    # Store predictions and ground truth for visualization later
    inputs_list: list[torch.Tensor] = []
    targets_list: list[torch.Tensor] = []
    outputs_list: list[torch.Tensor] = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            # Extract inputs and targets
            if isinstance(batch, dict):
inputs, targets = batch["image"], batch["mask"] # type:
ignore[reportUnknownVariableType]
            else:
                inputs, targets = batch  # type: ignore[reportUnknownVariableType]

            # Ensure inputs tensor has the correct shape (B, C, H, W)
            # If channels are last
            if (
                len(inputs.shape) == config.data.num_dims_image  # type: ignore[reportUnknownMemberType, reportUnknownArgumentType]
                and inputs.shape[-1]
config.data.num_channels_rgb  # type: ignore[reportUnknownMemberType]
            ):
permute(
         0, 3, 1, 2)

            # Ensure targets tensor has a channel dimension
            # If (B, H, W) without channel dimension
            if len(targets.shape) == config.data.num_dims_mask:  # type: ignore[reportUnknownMemberType, reportUnknownArgumentType]
                targets = targets.unsqueeze(1)  # type: ignore[reportUnknownVariableType, reportUnknownMemberType]

            # Handle numpy arrays which don't have .long() method
if hasattr(targets, "long"): # type: ignore[reportUnknownArgumentType]
                targets = (  # type: ignore[reportUnknownVariableType]
                    targets.long()  # type: ignore[reportUnknownVariableType]
                    if targets.dtype
torch.float32  # type: ignore[reportUnknownMemberType]
                    else targets  # type: ignore[reportUnknownVariableType]
                )
            else:
                # Convert numpy array to tensor if needed
tensor(
        targets, dtype=torch.float32)

            inputs = inputs.to(device)  # type: ignore[reportUnknownVariableType, reportUnknownMemberType]
            targets = targets.to(device)  # type: ignore[reportUnknownVariableType, reportUnknownMemberType]

            # Forward pass
            outputs = model(inputs)

            # Calculate metrics
            for name, metric_fn in metrics.items():
                metric_value = metric_fn(outputs, targets).item()
                results[f"test_{name}"] += metric_value

            # Store first few batches for visualization
            if batch_idx < config.evaluation.num_batches_visualize:
                inputs_list.append(inputs.cpu())  # type: ignore[reportUnknownMemberType, reportUnknownArgumentType]
                targets_list.append(targets.cpu())  # type: ignore[reportUnknownMemberType, reportUnknownArgumentType]
                outputs_list.append(outputs.cpu())

            # Log progress (dejar a la funciÃ³n llamadora)

    # Average metrics
    for key in results:
        results[key] /= len(dataloader)

    # Convert stored tensors to tensors
    all_inputs = torch.cat(inputs_list, dim=0)
    all_targets = torch.cat(targets_list, dim=0)
    all_outputs = torch.cat(outputs_list, dim=0)

    return results, (all_inputs, all_targets, all_outputs)
