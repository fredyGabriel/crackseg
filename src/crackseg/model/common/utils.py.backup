"""
Utility functions for U-Net model components. This module provides
utility functions for parameter counting, memory estimation, layer
hierarchy extraction, and configuration printing for U-Net models. For
spatial calculations, see spatial_utils module. For architecture
visualization, see visualization module.
"""

import  logging
from  typing  import   Any

import  torch

logger = logging.getLogger(__name__)


def count_parameters(model: torch.nn.Module) -> tuple[int, int]:
    """Count the trainable and non-trainable parameters in the model."""
trainable = sum(p.numel() for p in model.parameters() if
p.requires_grad) non_trainable = sum( p.numel() for p in
model.parameters() if not p.requires_grad ) return trainable,
non_trainable def estimate_receptive_field(encoder: Any) -> dict[str,
Any]:
"""Estimate the receptive field size of the model's encoder."""
    depth = getattr(encoder, "depth", None)
    if depth is not None:
        receptive_field_size = 3 + (depth * 4 * 2) - 1
        downsampling_factor = 2**depth
        return {
            "theoretical_rf_size": receptive_field_size,
            "downsampling_factor": downsampling_factor,
            "note": (
                "Theoretical estimate for standard U-Net with 3x3 kernels"
            ),
        }
    else:
        return {
            "note": (
                "Receptive field estimation requires a standard encoder "
                "with known depth"
            )
        }


def estimate_memory_usage(model: torch.nn.Module,
    encoder: Any,
    get_output_channels_fn: Any,
    input_shape: tuple[int, ...] | None = None,
) -> dict[str, Any]:
    """Estimate memory usage for the model."""
    param_bytes = sum(p.numel() * p.element_size() for p in model.parameters())
    buffer_bytes = sum(b.numel() * b.element_size() for b in model.buffers())
    model_size_mb = (param_bytes + buffer_bytes) / (1024 * 1024)

    if input_shape:
        B, _, H, W = input_shape  # C (channels) is unused
        depth = getattr(encoder, "depth", 4)
        encoder_memory = 0
        for i in range(depth):
            features = min(64 * (2**i), 512)
            h, w = H // (2**i), W // (2**i)
            encoder_memory += B * features * h * w * 4
        bottleneck_features = min(64 * (2**depth), 1024)
        bottleneck_h = H // (2**depth)
        bottleneck_w = W // (2**depth)
        bottleneck_memory = (
            B * bottleneck_features * bottleneck_h * bottleneck_w * 4
        )
        decoder_memory = 0
        for i in range(depth):
            j = depth - i - 1
            features = min(64 * (2**j), 512)
            h, w = H // (2**j), W // (2**j)
            decoder_memory += B * features * h * w * 4
        output_memory = B * get_output_channels_fn() * H * W * 4
        activation_mb = (
            encoder_memory + bottleneck_memory + decoder_memory + output_memory
        ) / (1024 * 1024)
        return {
            "model_size_mb": model_size_mb,
            "estimated_activation_mb": activation_mb,
            "total_estimated_mb": model_size_mb + activation_mb,
            "input_shape": input_shape,
        }
    return {
        "model_size_mb": model_size_mb,
        "note": "For activation memory estimates, provide input_shape",
    }


def get_layer_hierarchy(encoder: Any, bottleneck: Any, decoder: Any, final_activation: Any = None
) -> list[dict[str, Any]]:
    """Get the hierarchical structure of the model layers."""
    hierarchy: list[dict[str, Any]] = []
    encoder_info: dict[str, Any] = {
        "name": "Encoder",
        "type": encoder.__class__.__name__,
        "params": sum(p.numel() for p in encoder.parameters()),
        "out_channels": encoder.out_channels,
        "skip_channels": encoder.skip_channels,
    }
    if hasattr(encoder, "encoder_blocks"):
        encoder_blocks_info: list[dict[str, Any]] = []
        for i, block in enumerate(encoder.encoder_blocks):
            block_info = {
                "name": f"EncoderBlock_{i + 1}",
                "params": sum(p.numel() for p in block.parameters()),
                "in_channels": block.in_channels,
                "out_channels": block.out_channels,
            }
            encoder_blocks_info.append(block_info)
        encoder_info["blocks"] = encoder_blocks_info
    hierarchy.append(encoder_info)
    bottleneck_info: dict[str, Any] = {
        "name": "Bottleneck",
        "type": bottleneck.__class__.__name__,
        "params": sum(p.numel() for p in bottleneck.parameters()),
        "in_channels": bottleneck.in_channels,
        "out_channels": bottleneck.out_channels,
    }
    hierarchy.append(bottleneck_info)
    decoder_info: dict[str, Any] = {
        "name": "Decoder",
        "type": decoder.__class__.__name__,
        "params": sum(p.numel() for p in decoder.parameters()),
        "in_channels": decoder.in_channels,
        "out_channels": decoder.out_channels,
        "skip_channels": decoder.skip_channels,
    }
    if hasattr(decoder, "decoder_blocks"):
        decoder_blocks_info: list[dict[str, Any]] = []
        for i, block in enumerate(decoder.decoder_blocks):
            block_info = {
                "name": f"DecoderBlock_{i + 1}",
                "params": sum(p.numel() for p in block.parameters()),
                "in_channels": block.in_channels,
                "out_channels": block.out_channels,
            }
            decoder_blocks_info.append(block_info)
        if hasattr(decoder, "final_conv"):
            final_conv_info = {
                "name": "FinalConv",
                "params": sum(
                    p.numel() for p in decoder.final_conv.parameters()
                ),
                "in_channels": decoder.final_conv.in_channels,
                "out_channels": decoder.final_conv.out_channels,
            }
            decoder_blocks_info.append(final_conv_info)
        decoder_info["blocks"] = decoder_blocks_info
    hierarchy.append(decoder_info)
    if final_activation is not None:
        activation_info = {
            "name": "FinalActivation",
            "type": final_activation.__class__.__name__,
            "params": sum(p.numel() for p in final_activation.parameters()),
        }
        hierarchy.append(activation_info)
    return hierarchy


def print_config(config: dict[str, Any]) -> None:
    """Print the configuration with resolved paths."""
    for key, value in config.items():
        if isinstance(value, dict):
            print(f"{key}:")
            # Cast to ensure type safety for recursive call
            print_config(value)
        else:
            print(f"{key}: {value}")
